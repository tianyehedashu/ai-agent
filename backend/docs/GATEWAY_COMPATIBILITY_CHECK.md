# Gateway å±‚å…¼å®¹æ€§æ£€æŸ¥

## ä¸€ã€ä¸Šå±‚ä»£ç ä½¿ç”¨æƒ…å†µåˆ†æ

### 1.1 AgentEngine ä½¿ç”¨æƒ…å†µ

**æ–‡ä»¶ï¼š`backend/core/engine/agent.py`**

```python
# è°ƒç”¨
response = await self.llm.chat(
    messages=context,
    model=self.config.model,
    temperature=self.config.temperature,
    max_tokens=self.config.max_tokens,
    tools=tools,
)

# ä½¿ç”¨çš„å­—æ®µ
response.usage          # âœ… LLMResponse.usage: dict[str, int] | None
response.tool_calls     # âœ… LLMResponse.tool_calls: list[ToolCall] | None
response.content        # âœ… LLMResponse.content: str | None
response.finish_reason  # âœ… LLMResponse.finish_reason: str | None (è™½ç„¶ä»£ç ä¸­æ²¡ç”¨åˆ°ï¼Œä½†å·²å®šä¹‰)
```

**å…¼å®¹æ€§ï¼šâœ… å®Œå…¨å…¼å®¹**

### 1.2 ContextManager ä½¿ç”¨æƒ…å†µ

**æ–‡ä»¶ï¼š`backend/core/context/manager.py`**

```python
# è°ƒç”¨
response = await llm_gateway.chat(
    messages=[...],
    max_tokens=500,
)

# ä½¿ç”¨çš„å­—æ®µ
response.content  # âœ… LLMResponse.content: str | None
```

**å…¼å®¹æ€§ï¼šâœ… å®Œå…¨å…¼å®¹**

### 1.3 MemoryManager ä½¿ç”¨æƒ…å†µ

**æ–‡ä»¶ï¼š`backend/core/memory/manager.py`**

```python
# è°ƒç”¨
response = await self.llm.chat(
    messages=[{"role": "user", "content": prompt}],
    model=model,
    temperature=0.3,
)

# ä½¿ç”¨çš„å­—æ®µ
response.content  # âœ… LLMResponse.content: str | None
```

**å…¼å®¹æ€§ï¼šâœ… å®Œå…¨å…¼å®¹**

### 1.4 QualityFixer ä½¿ç”¨æƒ…å†µ

**æ–‡ä»¶ï¼š`backend/core/quality/fixer.py`**

```python
# è°ƒç”¨
response = await self.llm.chat(
    messages=[{"role": "user", "content": prompt}],
    model=None,
    temperature=0.3,
)

# ä½¿ç”¨çš„å­—æ®µ
response.content  # âœ… LLMResponse.content: str | None
```

**å…¼å®¹æ€§ï¼šâœ… å®Œå…¨å…¼å®¹**

### 1.5 æµå¼å“åº”ä½¿ç”¨æƒ…å†µ

**æ–‡ä»¶ï¼š`backend/core/llm/gateway.py` (å†…éƒ¨ä½¿ç”¨)**

```python
# æµå¼è°ƒç”¨
async for chunk in response:
    # ä½¿ç”¨çš„å­—æ®µ
    chunk.content        # âœ… StreamChunk.content: str | None
    chunk.tool_calls     # âœ… StreamChunk.tool_calls: list[dict[str, Any]] | None
    chunk.finish_reason  # âœ… StreamChunk.finish_reason: str | None
```

**å…¼å®¹æ€§ï¼šâœ… å®Œå…¨å…¼å®¹**

---

## äºŒã€è¿”å›ç±»å‹å¯¹æ¯”

### 2.1 éæµå¼å“åº”

**ä¹‹å‰ï¼ˆLiteLLM ç›´æ¥è¿”å›ï¼‰ï¼š**
```python
response = await acompletion(...)
# response æ˜¯ LiteLLM çš„å“åº”å¯¹è±¡
# response.choices[0].message.content
# response.choices[0].message.tool_calls
# response.usage
```

**ç°åœ¨ï¼ˆGateway è½¬æ¢åï¼‰ï¼š**
```python
response = await llm_gateway.chat(...)
# response æ˜¯ LLMResponse (å†…éƒ¨ç±»å‹)
# response.content          # ç›´æ¥è®¿é—®ï¼Œæ›´æ–¹ä¾¿
# response.tool_calls        # ç›´æ¥è®¿é—®ï¼Œå·²è½¬æ¢ä¸º ToolCall åˆ—è¡¨
# response.usage             # ç›´æ¥è®¿é—®ï¼Œå·²è½¬æ¢ä¸º dict
```

**ä¼˜åŠ¿ï¼š**
- âœ… æ¥å£æ›´ç®€æ´ï¼šä¸éœ€è¦ `response.choices[0].message.content`
- âœ… ç±»å‹å®‰å…¨ï¼šæ‰€æœ‰å­—æ®µéƒ½æœ‰æ˜ç¡®çš„ç±»å‹å®šä¹‰
- âœ… æ— å¯¹è±¡æ±¡æŸ“ï¼šä¸åŒ…å«ä»»ä½• LiteLLM å¯¹è±¡

### 2.2 æµå¼å“åº”

**ä¹‹å‰ï¼ˆLiteLLM ç›´æ¥è¿”å›ï¼‰ï¼š**
```python
async for chunk in response:
    # chunk æ˜¯ LiteLLM çš„æµå¼å“åº”å¯¹è±¡
    # chunk.choices[0].delta.content
    # chunk.choices[0].delta.tool_calls
```

**ç°åœ¨ï¼ˆGateway è½¬æ¢åï¼‰ï¼š**
```python
async for chunk in response:
    # chunk æ˜¯ StreamChunk (å†…éƒ¨ç±»å‹)
    # chunk.content        # ç›´æ¥è®¿é—®
    # chunk.tool_calls      # ç›´æ¥è®¿é—®ï¼Œå·²è½¬æ¢ä¸º dict åˆ—è¡¨
    # chunk.finish_reason   # ç›´æ¥è®¿é—®
```

**ä¼˜åŠ¿ï¼š**
- âœ… æ¥å£æ›´ç®€æ´ï¼šä¸éœ€è¦ `chunk.choices[0].delta.content`
- âœ… ç±»å‹å®‰å…¨ï¼šæ‰€æœ‰å­—æ®µéƒ½æœ‰æ˜ç¡®çš„ç±»å‹å®šä¹‰
- âœ… æ— å¯¹è±¡æ±¡æŸ“ï¼šä¸åŒ…å«ä»»ä½• LiteLLM å¯¹è±¡

---

## ä¸‰ã€åŠŸèƒ½å®Œæ•´æ€§æ£€æŸ¥

### 3.1 å·²å®ç°çš„åŠŸèƒ½

| åŠŸèƒ½ | çŠ¶æ€ | è¯´æ˜ |
|------|------|------|
| éæµå¼è°ƒç”¨ | âœ… | è¿”å› `LLMResponse` |
| æµå¼è°ƒç”¨ | âœ… | è¿”å› `AsyncGenerator[StreamChunk, None]` |
| å·¥å…·è°ƒç”¨ | âœ… | `response.tool_calls` å·²è½¬æ¢ä¸º `list[ToolCall]` |
| Token ä½¿ç”¨ç»Ÿè®¡ | âœ… | `response.usage` å·²è½¬æ¢ä¸º `dict[str, int]` |
| å®ŒæˆåŸå›  | âœ… | `response.finish_reason` å·²æå– |
| GLM ç‰¹æ®Šå­—æ®µ | âœ… | æ”¯æŒ `reasoning_content` |
| å¤šæ¨¡å‹æ”¯æŒ | âœ… | æ”¯æŒæ‰€æœ‰ LiteLLM æ”¯æŒçš„æ¨¡å‹ |
| é…ç½®ç®¡ç† | âœ… | ç»Ÿä¸€ç®¡ç† API Key å’Œæ¨¡å‹é…ç½® |

### 3.2 åŠŸèƒ½å¯¹æ¯”

| åŠŸèƒ½ | LiteLLM ç›´æ¥ä½¿ç”¨ | Gateway å°è£…å |
|------|-----------------|---------------|
| è®¿é—®å†…å®¹ | `response.choices[0].message.content` | `response.content` âœ… æ›´ç®€æ´ |
| è®¿é—®å·¥å…·è°ƒç”¨ | `response.choices[0].message.tool_calls` | `response.tool_calls` âœ… æ›´ç®€æ´ |
| è®¿é—®ä½¿ç”¨æƒ…å†µ | `response.usage` | `response.usage` âœ… ç›¸åŒ |
| ç±»å‹å®‰å…¨ | âŒ å¯èƒ½åŒ…å« LiteLLM å¯¹è±¡ | âœ… åªåŒ…å«åŸºæœ¬ç±»å‹ |
| åºåˆ—åŒ– | âŒ å¯èƒ½æœ‰é—®é¢˜ | âœ… å®Œå…¨å®‰å…¨ |

---

## å››ã€æ½œåœ¨é—®é¢˜æ£€æŸ¥

### 4.1 æ˜¯å¦æœ‰åŠŸèƒ½ä¸¢å¤±ï¼Ÿ

**æ£€æŸ¥é¡¹ï¼š**
- âœ… `content` - å·²ä¿ç•™
- âœ… `tool_calls` - å·²ä¿ç•™å¹¶è½¬æ¢
- âœ… `usage` - å·²ä¿ç•™å¹¶è½¬æ¢
- âœ… `finish_reason` - å·²ä¿ç•™
- âœ… æµå¼å“åº” - å·²ä¿ç•™
- âœ… GLM ç‰¹æ®Šå­—æ®µ - å·²æ”¯æŒ

**ç»“è®ºï¼šâœ… æ²¡æœ‰åŠŸèƒ½ä¸¢å¤±**

### 4.2 æ˜¯å¦æœ‰æ€§èƒ½é—®é¢˜ï¼Ÿ

**JSON åºåˆ—åŒ–/ååºåˆ—åŒ–ï¼š**
- æ€§èƒ½å½±å“ï¼šè½»å¾®ï¼ˆåªåœ¨ Gateway å±‚æ‰§è¡Œä¸€æ¬¡ï¼‰
- æ”¶ç›Šï¼šå®Œå…¨éš”ç¦» LiteLLM å¯¹è±¡ï¼Œé¿å…åºåˆ—åŒ–é—®é¢˜
- **ç»“è®ºï¼šâœ… æ€§èƒ½å½±å“å¯æ¥å—**

### 4.3 æ˜¯å¦æœ‰å…¼å®¹æ€§é—®é¢˜ï¼Ÿ

**ç±»å‹å…¼å®¹æ€§ï¼š**
- âœ… æ‰€æœ‰ä¸Šå±‚ä»£ç ä½¿ç”¨çš„å­—æ®µéƒ½å·²ä¿ç•™
- âœ… è¿”å›ç±»å‹ä¸ä¸Šå±‚ä»£ç æœŸæœ›ä¸€è‡´
- âœ… æ¥å£ç­¾åæ²¡æœ‰å˜åŒ–

**ç»“è®ºï¼šâœ… å®Œå…¨å…¼å®¹**

---

## äº”ã€æµ‹è¯•å»ºè®®

### 5.1 åŠŸèƒ½æµ‹è¯•

1. **éæµå¼è°ƒç”¨æµ‹è¯•**
   ```python
   response = await llm_gateway.chat(messages=[...])
   assert response.content is not None
   assert isinstance(response.usage, dict)
   ```

2. **æµå¼è°ƒç”¨æµ‹è¯•**
   ```python
   async for chunk in llm_gateway.chat(messages=[...], stream=True):
       assert chunk.content is not None or chunk.tool_calls is not None
   ```

3. **å·¥å…·è°ƒç”¨æµ‹è¯•**
   ```python
   response = await llm_gateway.chat(messages=[...], tools=[...])
   if response.tool_calls:
       assert isinstance(response.tool_calls, list)
       assert isinstance(response.tool_calls[0], ToolCall)
   ```

### 5.2 åºåˆ—åŒ–æµ‹è¯•

```python
# æµ‹è¯•åºåˆ—åŒ–æ˜¯å¦æ­£å¸¸
response = await llm_gateway.chat(messages=[...])
event = AgentEvent(
    type=EventType.TEXT,
    data={"response": response.model_dump()},
)
# åº”è¯¥æ²¡æœ‰ Pydantic è­¦å‘Š
serialized = event.model_dump(mode="json")
```

---

## å…­ã€æ€»ç»“

### âœ… å…¼å®¹æ€§ç»“è®º

**æ‰€æœ‰ä¸Šå±‚åŠŸèƒ½éƒ½èƒ½æ­£å¸¸ä½¿ç”¨ï¼**

1. **æ¥å£å…¼å®¹**ï¼šæ‰€æœ‰ä¸Šå±‚ä»£ç ä½¿ç”¨çš„å­—æ®µéƒ½å·²ä¿ç•™
2. **ç±»å‹å…¼å®¹**ï¼šè¿”å›ç±»å‹ä¸ä¸Šå±‚ä»£ç æœŸæœ›ä¸€è‡´
3. **åŠŸèƒ½å®Œæ•´**ï¼šæ²¡æœ‰åŠŸèƒ½ä¸¢å¤±
4. **æ€§èƒ½å¯æ¥å—**ï¼šJSON åºåˆ—åŒ–çš„æ€§èƒ½å½±å“å¾ˆå°
5. **æ›´å®‰å…¨**ï¼šå®Œå…¨éš”ç¦» LiteLLM å¯¹è±¡ï¼Œé¿å…åºåˆ—åŒ–é—®é¢˜

### ğŸ¯ æ”¹è¿›ç‚¹

1. **æ¥å£æ›´ç®€æ´**ï¼š`response.content` è€Œä¸æ˜¯ `response.choices[0].message.content`
2. **ç±»å‹æ›´å®‰å…¨**ï¼šæ‰€æœ‰å­—æ®µéƒ½æœ‰æ˜ç¡®çš„ç±»å‹å®šä¹‰
3. **æ— å¯¹è±¡æ±¡æŸ“**ï¼šä¸åŒ…å«ä»»ä½• LiteLLM å¯¹è±¡ï¼Œåºåˆ—åŒ–å®Œå…¨å®‰å…¨

### ğŸ“ å»ºè®®

1. **è¿è¡Œç°æœ‰æµ‹è¯•**ï¼šç¡®ä¿æ‰€æœ‰æµ‹è¯•é€šè¿‡
2. **æ£€æŸ¥æ—¥å¿—**ï¼šç¡®è®¤æ²¡æœ‰ Pydantic è­¦å‘Š
3. **åŠŸèƒ½éªŒè¯**ï¼šæµ‹è¯•å·¥å…·è°ƒç”¨ã€æµå¼å“åº”ç­‰åŠŸèƒ½
