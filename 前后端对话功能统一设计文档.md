# 前后端对话功能统一设计文档

## 一、当前问题分析

### 1.1 现状

**后端实现** (`backend/services/chat.py`):
- ✅ 已支持 SSE 流式响应
- ❌ 每次请求都要手动检索记忆（第118-156行）
- ❌ 记忆通过参数传递给 AgentEngine（第176行）
- ❌ 前端需要关心记忆管理

**前端实现** (`frontend/src/pages/chat/`):
- ✅ 已支持 SSE 流式接收
- ✅ 已支持流式内容显示（streamingContent）
- ❌ 需要了解后端记忆管理细节

### 1.2 问题

1. **记忆管理不透明**：前端不需要知道记忆的存在，应该由后端自动管理
2. **重复检索**：每次对话都要重新检索记忆，效率低
3. **接口复杂**：记忆作为参数传递，增加了接口复杂度
4. **流式响应不完整**：需要确认是否完全支持流式输出

---

## 二、设计目标

### 2.1 核心原则

1. **记忆自动管理**：后端根据 `session_id` 和 `user_id` 自动检索和管理记忆
2. **流式响应完整**：支持完整的流式输出，包括文本、工具调用、思考过程
3. **接口简化**：前端只需传递 `session_id` 和 `message`，无需关心记忆
4. **前后端统一**：前后端使用统一的事件类型和数据结构

### 2.2 架构设计

```
┌─────────────────────────────────────────────────────────────┐
│                    前端 (React)                              │
│  ┌──────────────────────────────────────────────────────┐   │
│  │  ChatInput: 发送 message + session_id                │   │
│  └──────────────────────────────────────────────────────┘   │
│                          │                                   │
│                          ▼                                   │
│  ┌──────────────────────────────────────────────────────┐   │
│  │  SSE Client: 接收流式事件                            │   │
│  │  - text: 文本片段                                    │   │
│  │  - thinking: 思考中                                  │   │
│  │  - tool_call: 工具调用                               │   │
│  │  - done: 完成                                       │   │
│  └──────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────┘
                          │
                          │ HTTP POST /api/v1/chat
                          │ SSE Stream Response
                          ▼
┌─────────────────────────────────────────────────────────────┐
│                    后端 (FastAPI)                            │
│  ┌──────────────────────────────────────────────────────┐   │
│  │  ChatService.chat()                                   │   │
│  │  - 自动检索记忆（基于 session_id + user_id）          │   │
│  │  - 自动管理会话                                       │   │
│  └──────────────────────────────────────────────────────┘   │
│                          │                                   │
│                          ▼                                   │
│  ┌──────────────────────────────────────────────────────┐   │
│  │  AgentEngine.run()                                   │   │
│  │  - 内部自动检索记忆（无需外部传递）                   │   │
│  │  - 流式调用 LLM                                      │   │
│  │  - 流式返回事件                                      │   │
│  └──────────────────────────────────────────────────────┘   │
│                          │                                   │
│                          ▼                                   │
│  ┌──────────────────────────────────────────────────────┐   │
│  │  ContextManager.build_context()                      │   │
│  │  - 自动检索长期记忆                                   │   │
│  │  - 自动加载会话历史                                   │   │
│  │  - 自动管理上下文窗口                                 │   │
│  └──────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────┘
```

---

## 三、后端改进方案

### 3.1 AgentEngine 改进

**当前实现** (`backend/core/engine/agent.py`):
```python
async def run(
    self,
    session_id: str,
    user_message: str,
    initial_state: AgentState | None = None,
    memories: list[str] | None = None,  # ❌ 需要外部传递
) -> AsyncGenerator[AgentEvent, None]:
    # ...
    context = self.context_manager.build_context(
        state.messages,
        memories=memories,  # ❌ 外部传递
    )
```

**改进后**:
```python
class AgentEngine:
    """Agent 执行引擎（自动记忆管理）"""

    def __init__(
        self,
        config: AgentConfig,
        llm_gateway: LLMGateway,
        tool_registry: ToolRegistry | None = None,
        checkpointer: Checkpointer | None = None,
        memory_manager: MemoryManager | None = None,  # ✅ 注入记忆管理器
        session_service: SessionService | None = None,  # ✅ 注入会话服务
    ) -> None:
        self.config = config
        self.llm = llm_gateway
        self.tools = tool_registry or ToolRegistry()
        self.checkpointer = checkpointer
        self.memory_manager = memory_manager  # ✅ 保存引用
        self.session_service = session_service  # ✅ 保存引用

        self.context_manager = ContextManager(
            config=config,
            max_context_tokens=100000,
            memory_manager=memory_manager,  # ✅ 传递给上下文管理器
            session_service=session_service,  # ✅ 传递给上下文管理器
        )

    async def run(
        self,
        session_id: str,
        user_message: str,
        user_id: str | None = None,  # ✅ 用于检索记忆
        initial_state: AgentState | None = None,
        # ❌ 移除 memories 参数，改为内部自动检索
    ) -> AsyncGenerator[AgentEvent, None]:
        """
        执行 Agent 主循环（自动记忆管理）

        Args:
            session_id: 会话 ID
            user_message: 用户消息
            user_id: 用户 ID（用于检索记忆，如果为 None 则从 session 获取）
            initial_state: 初始状态 (用于恢复)

        Yields:
            AgentEvent: 执行事件流
        """
        # 获取 user_id（如果未提供）
        if not user_id and self.session_service:
            session = await self.session_service.get_by_id(session_id)
            if session:
                user_id = str(session.user_id)

        # 初始化状态
        state = initial_state or AgentState(
            session_id=session_id,
            messages=[],
            iteration=0,
        )

        # 添加用户消息
        user_msg = Message(
            role=MessageRole.USER,
            content=user_message,
        )
        state.messages.append(user_msg)

        # 主执行循环
        while not state.completed:
            state.iteration += 1

            # 构建上下文（内部自动检索记忆）
            context = await self.context_manager.build_context(
                session_id=session_id,
                user_id=user_id,  # ✅ 用于自动检索记忆
                messages=state.messages,
                # ❌ 不再需要传递 memories
            )

            # 流式调用 LLM
            async for event in self._stream_llm_call(context, tools):
                yield event
```

### 3.2 ContextManager 改进

**当前实现** (`backend/core/context/manager.py`):
```python
def build_context(
    self,
    messages: list[Message],
    memories: list[str] | None = None,  # ❌ 需要外部传递
) -> Context:
    # ...
```

**改进后**:
```python
class ContextManager:
    """上下文管理器（自动记忆管理）"""

    def __init__(
        self,
        config: AgentConfig,
        max_context_tokens: int = 100000,
        memory_manager: MemoryManager | None = None,  # ✅ 注入
        session_service: SessionService | None = None,  # ✅ 注入
    ) -> None:
        self.config = config
        self.max_context_tokens = max_context_tokens
        self.memory_manager = memory_manager
        self.session_service = session_service
        self.counter = TokenCounter()

    async def build_context(
        self,
        session_id: str,
        user_id: str | None,
        messages: list[Message],
        # ❌ 移除 memories 参数
    ) -> Context:
        """
        构建上下文（自动检索记忆）

        Args:
            session_id: 会话 ID
            user_id: 用户 ID（用于检索记忆）
            messages: 当前消息列表

        Returns:
            上下文对象
        """
        context_messages = []
        available_tokens = self.max_context_tokens

        # 1. System Prompt
        system_prompt = self.config.system_prompt
        system_tokens = self.counter.count(system_prompt)
        context_messages.append({
            "role": "system",
            "content": system_prompt
        })
        available_tokens -= system_tokens

        # 2. 自动检索长期记忆（如果启用）
        if self.memory_manager and user_id:
            try:
                # 构建查询文本（结合当前消息和最近历史）
                query_text = messages[-1].content if messages else ""

                # 获取最近的历史消息作为上下文
                if self.session_service:
                    history_messages = await self.session_service.get_messages(
                        session_id=session_id,
                        limit=5
                    )
                    if history_messages:
                        recent_context = " ".join([
                            msg.content or ""
                            for msg in history_messages[-3:]
                            if msg.content
                        ])
                        query_text = f"{recent_context} {query_text}".strip()

                # 自动检索记忆
                memory_items = await self.memory_manager.search(
                    user_id=user_id,
                    query=query_text,
                    limit=10,  # 根据上下文窗口调整
                )

                if memory_items:
                    memory_content = self._format_memories(memory_items)
                    memory_tokens = self.counter.count(memory_content)

                    # 如果记忆不超过预算，添加到上下文
                    if memory_tokens <= available_tokens * 0.2:  # 记忆最多占 20%
                        context_messages.append({
                            "role": "system",
                            "content": f"相关记忆:\n{memory_content}"
                        })
                        available_tokens -= memory_tokens
            except Exception as e:
                logger.warning("Memory retrieval failed: %s", e, exc_info=True)

        # 3. 会话历史（自动加载）
        if self.session_service:
            history_messages = await self.session_service.get_messages(
                session_id=session_id,
                limit=50  # 根据上下文窗口调整
            )
            # 转换为消息格式并添加到上下文
            for msg in history_messages:
                msg_dict = {
                    "role": msg.role,
                    "content": msg.content or ""
                }
                msg_tokens = self.counter.count_messages([msg_dict])
                if msg_tokens <= available_tokens:
                    context_messages.append(msg_dict)
                    available_tokens -= msg_tokens
                else:
                    break  # 超出预算，停止添加

        # 4. 当前消息
        for msg in messages:
            msg_dict = {
                "role": msg.role.value if hasattr(msg.role, 'value') else str(msg.role),
                "content": msg.content
            }
            context_messages.append(msg_dict)

        return Context(
            messages=context_messages,
            total_tokens=self.counter.count_messages(context_messages),
        )

    def _format_memories(self, memory_items: list) -> str:
        """格式化记忆内容"""
        return "\n".join([
            f"- {item.content}"
            for item in memory_items
        ])
```

### 3.3 ChatService 简化

**当前实现** (`backend/services/chat.py`):
```python
async def chat(
    self,
    session_id: str | None,
    message: str,
    agent_id: str | None,
    user_id: str,
) -> AsyncGenerator[ChatEvent, None]:
    # ...
    # ❌ 手动检索记忆（第118-156行）
    memories: list[str] = []
    if self.memory_manager:
        memory_items = await self.memory_manager.search(...)
        memories = [m.content for m in memory_items]

    # ...
    # ❌ 传递记忆给 AgentEngine
    async for event in engine.run(
        session_id=session_id,
        user_message=message,
        memories=memories if memories else None,  # ❌
    ):
        yield chat_event
```

**改进后**:
```python
async def chat(
    self,
    session_id: str | None,
    message: str,
    agent_id: str | None,
    user_id: str,
) -> AsyncGenerator[ChatEvent, None]:
    """处理对话请求（自动记忆管理）"""

    # 创建或获取对话
    session = None
    if not session_id:
        session = await self.session_service.create(
            user_id=user_id,
            agent_id=agent_id,
        )
        session_id = str(session.id)
        await self.db.commit()
        yield ChatEvent(
            type="session_created",
            data={"session_id": session_id},
        )
    else:
        session = await self.session_service.get_by_id(session_id)
        if not session or str(session.user_id) != user_id:
            raise NotFoundError("Session", session_id)

    # 保存用户消息
    await self.session_service.add_message(
        session_id=session_id,
        role=MessageRole.USER,
        content=message,
    )

    # 获取 Agent 配置
    config = await self._get_agent_config(agent_id)

    # 创建执行引擎（注入记忆管理器和会话服务）
    engine = AgentEngine(
        config=config,
        llm_gateway=self.llm_gateway,
        tool_registry=self.tool_registry,
        checkpointer=self.checkpointer,
        memory_manager=self.memory_manager,  # ✅ 注入
        session_service=self.session_service,  # ✅ 注入
    )

    # 执行 Agent（不再传递记忆）
    final_content = ""
    conversation_for_memory: list[dict[str, str]] = []

    try:
        conversation_for_memory.append({"role": "user", "content": message})

        # ✅ 只传递 session_id 和 user_id，记忆自动管理
        async for event in engine.run(
            session_id=session_id,
            user_message=message,
            user_id=user_id,  # ✅ 用于自动检索记忆
            # ❌ 不再传递 memories
        ):
            chat_event = self._convert_event(event)
            yield chat_event

            # 收集流式内容
            if event.type == EventType.TEXT:
                text_content = event.data.get("content", "")
                if text_content:
                    final_content += text_content

        # 保存助手消息
        if final_content:
            await self.session_service.add_message(
                session_id=session_id,
                role=MessageRole.ASSISTANT,
                content=final_content,
            )
            conversation_for_memory.append({"role": "assistant", "content": final_content})

        # 自动提取记忆（后台任务，不阻塞响应）
        if self.memory_manager and conversation_for_memory:
            # 使用后台任务异步提取
            asyncio.create_task(self._extract_memories_async(
                session_id=session_id,
                user_id=user_id,
                conversation=conversation_for_memory,
            ))

    except Exception as e:
        logger.error("Chat error: %s", e, exc_info=True)
        yield ChatEvent(
            type="error",
            data={"error": str(e), "session_id": session_id},
        )

    async def _extract_memories_async(
        self,
        session_id: str,
        user_id: str,
        conversation: list[dict[str, str]],
    ):
        """异步提取记忆（后台任务）"""
        try:
            await self.memory_manager.extract_memories(
                session_id=session_id,
                user_id=user_id,
                conversation=conversation,
            )
        except Exception as e:
            logger.warning("Memory extraction failed: %s", e, exc_info=True)
```

---

## 四、流式响应完整实现

### 4.1 后端流式实现

**LLM Gateway 流式调用** (`backend/core/llm/gateway.py`):
```python
async def _stream_chat(self, **kwargs: Any) -> AsyncGenerator[StreamChunk, None]:
    """流式调用"""
    model = kwargs.get("model", self.config.default_model)
    kwargs["stream"] = True

    try:
        response = await acompletion(**kwargs)

        tool_calls_buffer: dict[int, dict[str, Any]] = {}

        async for chunk in response:
            chunk_dict = self._extract_chunk_dict(chunk)
            stream_chunk = self._process_stream_chunk(chunk_dict, tool_calls_buffer)

            if stream_chunk:
                yield stream_chunk

    except Exception as e:
        logger.error("Stream LLM call failed: %s", e)
        raise
```

**AgentEngine 流式处理**:
```python
async def _stream_llm_call(
    self,
    context: Context,
    tools: list[dict] | None,
) -> AsyncGenerator[AgentEvent, None]:
    """流式调用 LLM 并处理响应"""

    # 流式调用
    stream = self.llm.chat(
        messages=context.messages,
        tools=tools,
        model=self.config.model,
        temperature=self.config.temperature,
        stream=True,  # ✅ 启用流式
    )

    full_content = ""
    tool_calls: list[ToolCall] = []

    async for chunk in stream:
        # 文本片段
        if chunk.content:
            full_content += chunk.content
            yield AgentEvent(
                type=EventType.TEXT,
                data={"content": chunk.content},
            )

        # 工具调用
        if chunk.tool_calls:
            for tc in chunk.tool_calls:
                tool_calls.append(tc)
                yield AgentEvent(
                    type=EventType.TOOL_CALL,
                    data={
                        "tool_name": tc.name,
                        "arguments": tc.arguments,
                    },
                )

    # 如果有工具调用，执行工具
    if tool_calls:
        for tc in tool_calls:
            result = await self.tools.execute(tc.name, tc.arguments)
            yield AgentEvent(
                type=EventType.TOOL_RESULT,
                data={
                    "tool_name": tc.name,
                    "result": result.content,
                    "success": result.success,
                },
            )

    # 完成事件
    yield AgentEvent(
        type=EventType.DONE,
        data={
            "final_message": {
                "role": "assistant",
                "content": full_content,
            },
            "tool_calls": [tc.dict() for tc in tool_calls],
        },
    )
```

### 4.2 前端流式接收

**SSE 客户端** (`frontend/src/lib/api/chat.ts`):
```typescript
export interface ChatEvent {
  type: 'session_created' | 'thinking' | 'text' | 'tool_call' | 'tool_result' | 'done' | 'error'
  data: any
  timestamp?: string
}

export async function* streamChat(
  message: string,
  sessionId: string | null,
  agentId?: string
): AsyncGenerator<ChatEvent, void, unknown> {
  const response = await fetch('/api/v1/chat', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${getToken()}`,
    },
    body: JSON.stringify({
      message,
      session_id: sessionId,
      agent_id: agentId,
    }),
  })

  if (!response.ok) {
    throw new Error(`Chat request failed: ${response.statusText}`)
  }

  const reader = response.body?.getReader()
  const decoder = new TextDecoder()

  if (!reader) {
    throw new Error('Response body is not readable')
  }

  let buffer = ''

  while (true) {
    const { done, value } = await reader.read()

    if (done) {
      break
    }

    buffer += decoder.decode(value, { stream: true })
    const lines = buffer.split('\n')
    buffer = lines.pop() || '' // 保留最后一个不完整的行

    for (const line of lines) {
      if (line.startsWith('data: ')) {
        const data = line.slice(6) // 移除 'data: ' 前缀

        if (data === '[DONE]') {
          return
        }

        try {
          const event: ChatEvent = JSON.parse(data)
          yield event
        } catch (e) {
          console.error('Failed to parse SSE event:', e)
        }
      }
    }
  }
}
```

**React Hook** (`frontend/src/hooks/useChat.ts`):
```typescript
import { useState, useCallback } from 'react'
import { streamChat, type ChatEvent } from '@/lib/api/chat'
import type { Message } from '@/types'

export function useChat() {
  const [messages, setMessages] = useState<Message[]>([])
  const [streamingContent, setStreamingContent] = useState('')
  const [isLoading, setIsLoading] = useState(false)
  const [sessionId, setSessionId] = useState<string | null>(null)

  const sendMessage = useCallback(async (message: string, agentId?: string) => {
    setIsLoading(true)
    setStreamingContent('')

    try {
      for await (const event of streamChat(message, sessionId, agentId)) {
        switch (event.type) {
          case 'session_created':
            setSessionId(event.data.session_id)
            break

          case 'text':
            // 累积流式文本
            setStreamingContent((prev) => prev + event.data.content)
            break

          case 'tool_call':
            // 显示工具调用
            console.log('Tool call:', event.data)
            break

          case 'done':
            // 完成，保存完整消息
            const fullContent = streamingContent || event.data.final_message?.content || ''
            if (fullContent) {
              setMessages((prev) => [
                ...prev,
                {
                  id: Date.now().toString(),
                  role: 'assistant',
                  content: fullContent,
                  createdAt: new Date().toISOString(),
                },
              ])
              setStreamingContent('')
            }
            setIsLoading(false)
            break

          case 'error':
            console.error('Chat error:', event.data.error)
            setIsLoading(false)
            break
        }
      }
    } catch (error) {
      console.error('Chat stream error:', error)
      setIsLoading(false)
    }
  }, [sessionId, streamingContent])

  return {
    messages,
    streamingContent,
    isLoading,
    sessionId,
    sendMessage,
  }
}
```

---

## 五、API 接口设计

### 5.1 请求格式

```typescript
POST /api/v1/chat
Content-Type: application/json
Authorization: Bearer <token>

{
  "message": "用户消息",
  "session_id": "会话ID（可选，不提供则创建新会话）",
  "agent_id": "Agent ID（可选）"
}
```

### 5.2 响应格式（SSE）

```
Content-Type: text/event-stream
Cache-Control: no-cache
Connection: keep-alive

data: {"type":"session_created","data":{"session_id":"xxx"},"timestamp":"2024-01-01T00:00:00Z"}

data: {"type":"thinking","data":{"iteration":1,"status":"processing"},"timestamp":"2024-01-01T00:00:01Z"}

data: {"type":"text","data":{"content":"这是"},"timestamp":"2024-01-01T00:00:02Z"}

data: {"type":"text","data":{"content":"一段"},"timestamp":"2024-01-01T00:00:02Z"}

data: {"type":"text","data":{"content":"流式"},"timestamp":"2024-01-01T00:00:03Z"}

data: {"type":"text","data":{"content":"文本"},"timestamp":"2024-01-01T00:00:03Z"}

data: {"type":"done","data":{"final_message":{"role":"assistant","content":"这是一段流式文本"}},"timestamp":"2024-01-01T00:00:04Z"}

data: [DONE]
```

### 5.3 事件类型

| 事件类型 | 说明 | 数据格式 |
|---------|------|---------|
| `session_created` | 新会话创建 | `{session_id: string}` |
| `thinking` | Agent 思考中 | `{iteration: number, status: string}` |
| `text` | 文本片段（流式） | `{content: string}` |
| `tool_call` | 工具调用 | `{tool_name: string, arguments: object}` |
| `tool_result` | 工具执行结果 | `{tool_name: string, result: any, success: boolean}` |
| `done` | 完成 | `{final_message: {role: string, content: string}}` |
| `error` | 错误 | `{error: string}` |

---

## 六、迁移计划

### 6.1 阶段 1: 后端改进（2-3 天）

1. **更新 ContextManager**:
   - 添加 `memory_manager` 和 `session_service` 参数
   - 实现自动记忆检索
   - 移除 `memories` 参数

2. **更新 AgentEngine**:
   - 添加 `memory_manager` 和 `session_service` 参数
   - 移除 `memories` 参数
   - 传递 `user_id` 给 ContextManager

3. **更新 ChatService**:
   - 移除手动记忆检索代码
   - 注入 `memory_manager` 和 `session_service` 到 AgentEngine
   - 简化接口

### 6.2 阶段 2: 流式响应完善（1-2 天）

1. **验证流式实现**:
   - 测试 LLM Gateway 流式输出
   - 测试 AgentEngine 流式处理
   - 测试 SSE 事件流

2. **优化流式性能**:
   - 减少延迟
   - 优化缓冲区管理
   - 处理网络中断

### 6.3 阶段 3: 前端优化（1-2 天）

1. **更新前端代码**:
   - 使用新的 API 接口
   - 优化流式内容显示
   - 处理各种事件类型

2. **用户体验优化**:
   - 流式文本动画
   - 工具调用可视化
   - 错误处理

### 6.4 阶段 4: 测试和优化（1-2 天）

1. **功能测试**:
   - 测试记忆自动检索
   - 测试流式响应
   - 测试会话管理

2. **性能测试**:
   - 测试记忆检索性能
   - 测试流式响应延迟
   - 优化慢查询

---

## 七、总结

### 7.1 改进点

1. **记忆自动管理**：
   - ✅ 后端自动检索，前端无需关心
   - ✅ 减少接口复杂度
   - ✅ 提高代码可维护性

2. **流式响应完整**：
   - ✅ 支持完整的流式输出
   - ✅ 实时显示文本片段
   - ✅ 支持工具调用和结果

3. **前后端统一**：
   - ✅ 统一的事件类型
   - ✅ 统一的数据格式
   - ✅ 清晰的职责划分

### 7.2 注意事项

1. **向后兼容**：保持 API 接口向后兼容，逐步迁移
2. **错误处理**：完善的错误处理和降级策略
3. **性能优化**：记忆检索和流式响应的性能优化
4. **文档更新**：更新 API 文档和使用示例
