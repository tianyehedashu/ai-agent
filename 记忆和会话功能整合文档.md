# 记忆和会话功能整合文档

> **更新日期**: 2026-01-17
> **主文档**: [上下文管理架构设计](backend/docs/CONTEXT_MANAGEMENT_ARCHITECTURE.md)
>
> ⚠️ 本文档为历史设计文档，最新架构设计请参阅上方主文档。

---

## 〇、调用流程概述

### 每次对话的调用流程

```
用户消息 → ChatService.chat()
    │
    ├─→ LangGraphAgentEngine.run()
    │       ├─→ recall_memory (向量检索，每次执行)
    │       ├─→ call_llm (LLM 推理，必须执行)
    │       └─→ extract_memory (条件触发：≥4条消息)
    │
    └─→ SimpleMem.process_and_store() (后台异步，条件触发)
            ├─→ 信息密度过滤 (novelty_threshold)
            ├─→ LLM 提取原子记忆 (extraction_model)
            └─→ 向量存储 + BM25 索引
```

### 关键配置项

```toml
# config/app.toml
[simplemem]
enabled = true
extraction_model = "dashscope/qwen-turbo"  # 小模型，成本低

[simplemem.filter]
novelty_threshold = 0.35   # 信息密度阈值
skip_trivial = true        # 跳过简单问答
```

### 是否合理？

**结论：基本合理，已优化。**

1. ✅ SimpleMem 已实现业界推荐的混合检索（向量 + BM25 + RRF 融合）
2. ✅ 有信息密度过滤（存储阶段），不是真的"每次都调用模型"
3. ✅ 使用小模型做记忆提取，成本可控
4. ✅ **已优化**：统一记忆提取入口，避免重复
5. ✅ 检索阶段不跳过任何消息（短消息如 "?" "不对" 也需要上下文）

---

## 〇.1 统一记忆提取设计（2026-01-17 更新）

### 问题

原有设计中存在两个记忆提取入口：
- `LangGraphAgentEngine._extract_memory` → 调用 `MemoryExtractor`
- `ChatService.chat` → 后台调用 `SimpleMem.process_and_store`

两者可能对同一对话重复提取相同的信息。

### 解决方案

**统一使用 SimpleMem，禁用 extract_memory 节点**

```python
# backend/core/engine/langgraph_agent.py
async def _extract_memory(self, state: AgentState):
    # 如果启用了 SimpleMem，跳过此节点（由 ChatService 异步处理）
    if settings.simplemem_enabled:
        logger.debug("Skipping extract_memory: SimpleMem is enabled")
        return {}
    # ... 否则使用 MemoryExtractor
```

### 为什么选择 SimpleMem？

| 对比项 | MemoryExtractor | SimpleMem |
|--------|----------------|-----------|
| 过滤机制 | 无 | ✅ novelty_threshold + skip_trivial |
| 检索方式 | 仅向量 | ✅ BM25 + 向量 + RRF 融合 |
| 执行方式 | 同步（在图内） | ✅ 异步（后台任务） |
| 对响应影响 | 增加延迟 | ✅ 零延迟 |

### 检索阶段设计

**检索阶段不跳过任何消息**，因为：
- 短消息如 `?` `不对` `继续` 可能需要上下文理解用户意图
- 向量检索成本很低（毫秒级），不值得冒险跳过
- SimpleMem 的过滤在**存储阶段**已完成，检索阶段应尽量召回

---

## 一、当前实现问题分析

### 1.1 现状

当前项目的记忆和会话功能实现情况：

- **记忆管理** (`backend/core/memory/manager.py`): 自定义实现，使用 LLM 提取记忆，存储到 PostgreSQL + 向量数据库
- **会话管理** (`backend/services/session.py`): 自定义实现，使用 PostgreSQL 存储会话和消息
- **检查点管理** (`backend/core/engine/checkpointer.py`): 自定义实现，使用 Redis 存储检查点

### 1.2 问题

1. **未使用 LangChain 记忆组件**: 项目已安装 `langchain-openai>=1.0.0`，但未使用 LangChain 提供的记忆类型（如 `ConversationBufferMemory`, `ConversationSummaryMemory` 等）
2. **未使用 LangGraph 检查点机制**: 项目已安装 `langgraph>=1.0.0`，但未使用 LangGraph 原生的 `checkpointer` 功能
3. **未使用 LangGraph Store**: 未使用 LangGraph 提供的 `Store` 接口实现长期记忆
4. **功能重复**: 自定义实现与 LangChain/LangGraph 提供的功能重复，维护成本高

### 1.3 影响

- 无法利用 LangChain/LangGraph 的优化和最佳实践
- 代码维护成本高，需要自己处理各种边界情况
- 与 LangGraph 生态集成困难，无法使用预构建的组件

---

## 二、LangChain/LangGraph 提供的功能

### 2.1 LangChain 记忆类型

LangChain 提供了多种记忆类型，适用于不同场景：

| 记忆类型 | 用途 | 特点 |
|---------|------|------|
| `ConversationBufferMemory` | 保存完整对话历史 | 简单直接，但可能超出上下文窗口 |
| `ConversationSummaryMemory` | 保存对话摘要 | 自动摘要，节省 token，但可能丢失细节 |
| `ConversationBufferWindowMemory` | 保存最近 N 条消息 | 固定窗口大小，自动截断旧消息 |
| `ConversationSummaryBufferMemory` | 结合摘要和窗口 | 保留最近消息 + 摘要旧消息 |
| `ConversationKGMemory` | 知识图谱记忆 | 提取实体和关系，结构化存储 |
| `ConversationEntityMemory` | 实体记忆 | 记住对话中提到的实体信息 |

### 2.2 LangGraph 检查点（Checkpointer）

LangGraph 的检查点机制用于管理**短期记忆**（会话内状态）：

```python
from langgraph.checkpoint.memory import MemorySaver
from langgraph.checkpoint.postgres import PostgresSaver
from langgraph.checkpoint.redis import RedisSaver

# 内存检查点（开发测试）
checkpointer = MemorySaver()

# PostgreSQL 检查点（生产环境）
checkpointer = PostgresSaver.from_conn_string("postgresql://...")

# Redis 检查点（高性能场景）
checkpointer = RedisSaver.from_client(redis_client)
```

**功能特性**：
- 自动保存和恢复图执行状态
- 支持多线程（thread_id）隔离
- 支持状态版本管理
- 支持中断和恢复执行

### 2.3 LangGraph Store（长期记忆）

LangGraph 的 Store 用于管理**长期记忆**（跨会话记忆）：

```python
from langgraph.store.memory import InMemoryStore
from langgraph.store.postgres import PostgresStore

# 内存存储（开发测试）
store = InMemoryStore()

# PostgreSQL 存储（生产环境，用于元数据和 JSON 文档）
# 注意：向量搜索使用独立的向量数据库（Qdrant/Chroma）
store = PostgresStore.from_conn_string(
    "postgresql://..."
)
await store.setup()  # 初始化表结构
```

**功能特性**：
- 支持命名空间（namespace）组织数据
- 支持 JSON 文档存储
- 支持多用户隔离
- **注意**：向量搜索应使用独立的向量数据库（Qdrant/Chroma），而不是 PostgreSQL 的向量扩展

### 2.4 LangMem SDK（可选）

LangMem SDK 提供了更高级的记忆管理功能：

- **记忆提取**: 自动从对话中提取重要信息
- **记忆分类**: 区分语义记忆、情景记忆、程序记忆
- **记忆更新**: 智能更新和合并记忆

---

## 三、整合方案

### 3.1 架构设计

```
┌─────────────────────────────────────────────────────────────┐
│                    Agent 执行流程                            │
└─────────────────────────────────────────────────────────────┘
                            │
                            ▼
        ┌───────────────────────────────────────┐
        │   1. 用户输入 + 会话 ID (thread_id)    │
        └───────────────────────────────────────┘
                            │
                            ▼
        ┌───────────────────────────────────────┐
        │   2. LangGraph Checkpointer            │
        │      恢复会话状态（短期记忆）           │
        │      - 对话历史                        │
        │      - Agent 状态                      │
        │      存储: PostgreSQL                  │
        └───────────────────────────────────────┘
                            │
                            ▼
        ┌───────────────────────────────────────┐
        │   3. 长期记忆检索（混合架构）          │
        │      ┌────────────────────────────┐   │
        │      │ 向量搜索 (Qdrant/Chroma)  │   │
        │      │  - 语义相似度搜索          │   │
        │      │  - 使用 LiteLLM 生成嵌入   │   │
        │      └────────────────────────────┘   │
        │      ┌────────────────────────────┐   │
        │      │ LangGraph Store (PostgreSQL)│   │
        │      │  - 元数据存储               │   │
        │      │  - JSON 文档                │   │
        │      │  - 不用于向量搜索           │   │
        │      └────────────────────────────┘   │
        └───────────────────────────────────────┘
                            │
                            ▼
        ┌───────────────────────────────────────┐
        │   4. LangChain Memory                  │
        │      管理对话上下文                    │
        │      - ConversationSummaryMemory      │
        │      - 使用 LiteLLM 适配器            │
        └───────────────────────────────────────┘
                            │
                            ▼
        ┌───────────────────────────────────────┐
        │   5. LiteLLM Gateway                  │
        │      - 统一多模型接口                 │
        │      - 支持 Fallback/重试             │
        │      - 工具执行                      │
        └───────────────────────────────────────┘
                            │
                            ▼
        ┌───────────────────────────────────────┐
        │   6. 更新记忆                          │
        │      - 保存到 Checkpointer (短期)      │
        │      - 提取并保存到:                  │
        │        • Store (元数据)               │
        │        • 向量数据库 (向量)             │
        └───────────────────────────────────────┘
```

### 3.2 架构说明

**核心原则**：
1. **LiteLLM 统一 LLM 调用**: 所有 LLM 调用（包括对话、摘要、嵌入）都通过 LiteLLM Gateway
2. **向量搜索独立**: 使用专用向量数据库（Qdrant/Chroma），不耦合到 PostgreSQL
3. **Store 仅存元数据**: LangGraph Store 用于存储 JSON 文档和元数据，不承担向量搜索
4. **混合架构优势**:
   - 向量搜索性能更好（专用数据库）
   - 元数据查询更灵活（PostgreSQL）
   - 各组件职责清晰，易于维护

### 3.2 实现步骤

#### 步骤 1: 替换检查点实现

**当前实现** (`backend/core/engine/checkpointer.py`):
```python
class Checkpointer:
    def __init__(self, storage: CheckpointStorage):
        self.storage = storage
    # ... 自定义实现
```

**新实现** (使用 LangGraph):
```python
from langgraph.checkpoint.postgres import PostgresSaver
from langgraph.checkpoint.redis import RedisSaver

class Checkpointer:
    """使用 LangGraph 的检查点实现"""

    def __init__(self, storage_type: str = "postgres"):
        if storage_type == "postgres":
            self.checkpointer = PostgresSaver.from_conn_string(
                settings.DATABASE_URL
            )
        elif storage_type == "redis":
            self.checkpointer = RedisSaver.from_client(
                await get_redis_client()
            )
        else:
            from langgraph.checkpoint.memory import MemorySaver
            self.checkpointer = MemorySaver()

    async def save(self, thread_id: str, state: dict):
        """保存检查点"""
        # LangGraph 会自动处理
        pass

    async def load(self, thread_id: str) -> dict:
        """加载检查点"""
        # LangGraph 会自动处理
        pass
```

#### 步骤 2: 集成 LangGraph Store + 向量数据库

**新文件**: `backend/core/memory/langgraph_store.py`

```python
from langgraph.store.postgres import PostgresStore
from core.llm.gateway import LLMGateway
from db.vector import VectorStore
from app.config import settings
import litellm  # 用于生成嵌入向量

class LongTermMemoryStore:
    """
    长期记忆存储（混合架构）

    - LangGraph Store: 存储元数据和 JSON 文档（PostgreSQL）
    - VectorStore: 向量搜索（Qdrant/Chroma）
    - LiteLLM: 生成嵌入向量
    """

    def __init__(
        self,
        llm_gateway: LLMGateway,
        vector_store: VectorStore,
    ):
        self.llm_gateway = llm_gateway
        self.vector_store = vector_store

        # 创建 LangGraph Store（仅用于元数据，不用于向量搜索）
        self.store = PostgresStore.from_conn_string(
            settings.DATABASE_URL
        )

    async def setup(self):
        """初始化 Store 和向量集合"""
        await self.store.setup()

        # 确保向量集合存在
        await self.vector_store.create_collection(
            name="memories",
            dimension=1536  # text-embedding-3-small 的维度
        )

    async def search(
        self,
        user_id: str,
        query: str,
        limit: int = 10,
        memory_type: str | None = None,
    ) -> list[dict]:
        """
        搜索长期记忆（使用向量搜索）

        Args:
            user_id: 用户 ID
            query: 查询文本
            limit: 返回数量
            memory_type: 记忆类型过滤

        Returns:
            记忆列表
        """
        # 1. 向量搜索（使用 Qdrant/Chroma）
        # VectorStore.search 内部会生成查询向量，如果需要统一使用 LiteLLM，可以：
        # query_embedding = await litellm.aembedding(
        #     model=self.llm_gateway.config.embedding_model,
        #     input=query
        # )
        # 然后修改 VectorStore 接口支持直接传递向量

        vector_results = await self.vector_store.search(
            collection="memories",
            query=query,  # VectorStore 内部会使用 OpenAI 生成嵌入，未来可改为 LiteLLM
            limit=limit * 2,  # 多取一些，后面过滤
            query_filter={"user_id": user_id}
        )

        # 2. 从 LangGraph Store 获取完整元数据
        memories = []
        namespace = [f"user_{user_id}", "memories"]

        for result in vector_results:
            memory_id = result["id"]

            # 从 Store 获取元数据
            if memory_type:
                namespace_with_type = [*namespace, memory_type]
            else:
                namespace_with_type = namespace

            # 尝试从不同命名空间获取
            memory_data = None
            for ns in [namespace_with_type, namespace]:
                try:
                    memory_data = await self.store.get(
                        namespace=ns,
                        key=memory_id
                    )
                    if memory_data:
                        break
                except Exception:
                    continue

            if memory_data:
                value = memory_data.value if hasattr(memory_data, 'value') else memory_data
                # 类型过滤
                if memory_type and value.get("type") != memory_type:
                    continue

                memories.append({
                    "id": memory_id,
                    "content": value.get("content", result.get("text", "")),
                    "type": value.get("type"),
                    "importance": value.get("importance", 0),
                    "metadata": value.get("metadata", {}),
                    "score": result.get("score", 0),
                })

        # 3. 按分数和重要性排序
        memories.sort(key=lambda x: (x["score"], x["importance"]), reverse=True)
        return memories[:limit]

    async def put(
        self,
        user_id: str,
        memory_type: str,
        content: str,
        importance: float = 5.0,
        metadata: dict | None = None,
    ) -> str:
        """
        存储长期记忆（同时存储到 Store 和向量数据库）

        Args:
            user_id: 用户 ID
            memory_type: 记忆类型
            content: 记忆内容
            importance: 重要性
            metadata: 元数据

        Returns:
            记忆 ID
        """
        import uuid
        from datetime import UTC, datetime

        memory_id = str(uuid.uuid4())
        namespace = [f"user_{user_id}", "memories", memory_type]

        # 1. 存储到 LangGraph Store（元数据）
        value = {
            "content": content,
            "type": memory_type,
            "importance": importance,
            "metadata": metadata or {},
            "created_at": datetime.now(UTC).isoformat(),
        }

        await self.store.put(
            namespace=namespace,
            key=memory_id,
            value=value
        )

        # 2. 存储到向量数据库（用于语义搜索）
        # 注意：VectorStore.upsert 会自动生成嵌入，但可以通过传递 vector 参数使用 LiteLLM 生成的嵌入
        # 如果需要统一使用 LiteLLM，可以先生成嵌入：
        # import litellm
        # embedding = await litellm.aembedding(
        #     model=self.llm_gateway.config.embedding_model,
        #     input=content
        # )
        # vector = embedding.data[0]["embedding"]

        await self.vector_store.upsert(
            collection="memories",
            point_id=memory_id,
            text=content,
            metadata={
                "user_id": user_id,
                "memory_type": memory_type,
                "importance": importance,
                **(metadata or {}),
            }
            # vector=vector  # 如果使用 LiteLLM 生成的嵌入，传递这里
        )

        return memory_id

    async def delete(self, user_id: str, memory_id: str, memory_type: str):
        """删除记忆（同时从 Store 和向量数据库删除）"""
        namespace = [f"user_{user_id}", "memories", memory_type]

        # 从 Store 删除
        await self.store.delete(namespace=namespace, key=memory_id)

        # 从向量数据库删除
        await self.vector_store.delete(
            collection="memories",
            point_ids=[memory_id]
        )
```

#### 步骤 3: 集成 LangChain Memory（适配 LiteLLM）

**新文件**: `backend/core/memory/langchain_memory.py`

```python
from langchain.memory import ConversationSummaryMemory, ConversationBufferWindowMemory
from langchain_core.messages import BaseMessage
from langchain_core.language_models import BaseChatModel
from core.llm.gateway import LLMGateway

class LiteLLMChatModelAdapter(BaseChatModel):
    """
    LiteLLM 适配器，使其兼容 LangChain 接口

    将 LiteLLM Gateway 包装成 LangChain 的 BaseChatModel
    """

    def __init__(self, llm_gateway: LLMGateway):
        super().__init__()
        self.llm_gateway = llm_gateway

    def _generate(self, messages, stop=None, run_manager=None, **kwargs):
        """同步生成（LangChain Memory 可能需要）"""
        # 注意：LiteLLM Gateway 是异步的，这里需要适配
        import asyncio
        return asyncio.run(self._agenerate(messages, stop, run_manager, **kwargs))

    async def _agenerate(self, messages, stop=None, run_manager=None, **kwargs):
        """异步生成"""
        from langchain_core.messages import HumanMessage, AIMessage
        from langchain_core.outputs import ChatGeneration, ChatResult

        # 转换消息格式
        lite_messages = []
        for msg in messages:
            if isinstance(msg, HumanMessage):
                lite_messages.append({"role": "user", "content": msg.content})
            elif isinstance(msg, AIMessage):
                lite_messages.append({"role": "assistant", "content": msg.content})

        # 调用 LiteLLM Gateway
        response = await self.llm_gateway.chat(messages=lite_messages)

        # 转换为 LangChain 格式
        ai_message = AIMessage(content=response.content or "")
        generation = ChatGeneration(message=ai_message)

        return ChatResult(generations=[generation])

    @property
    def _llm_type(self) -> str:
        return "litellm_adapter"

class ConversationMemory:
    """
    对话记忆管理（使用 LangChain Memory + LiteLLM）

    注意：需要将 LiteLLM Gateway 适配为 LangChain 接口
    """

    def __init__(
        self,
        llm_gateway: LLMGateway,
        memory_type: str = "summary"
    ):
        """
        初始化对话记忆

        Args:
            llm_gateway: LiteLLM Gateway 实例
            memory_type: 记忆类型 (summary/buffer/window)
        """
        # 创建适配器
        llm_adapter = LiteLLMChatModelAdapter(llm_gateway)

        if memory_type == "summary":
            self.memory = ConversationSummaryMemory(
                llm=llm_adapter,
                return_messages=True,
                memory_key="history"
            )
        elif memory_type == "window":
            self.memory = ConversationBufferWindowMemory(
                k=10,  # 保留最近 10 条消息
                return_messages=True,
                memory_key="history"
            )
        else:
            from langchain.memory import ConversationBufferMemory
            self.memory = ConversationBufferMemory(
                return_messages=True,
                memory_key="history"
            )

    def load_memory_variables(self, inputs: dict) -> dict:
        """加载记忆变量"""
        return self.memory.load_memory_variables(inputs)

    def save_context(self, inputs: dict, outputs: dict):
        """保存上下文"""
        self.memory.save_context(inputs, outputs)

    def clear(self):
        """清空记忆"""
        self.memory.clear()
```

#### 步骤 4: 更新 Agent Engine

**更新**: `backend/core/engine/agent.py`

```python
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.postgres import PostgresSaver
from core.memory.langgraph_store import LongTermMemoryStore
from core.memory.langchain_memory import ConversationMemory
from core.llm.gateway import LLMGateway
from db.vector import VectorStore

class AgentEngine:
    """Agent 执行引擎（集成 LangGraph/LangChain + LiteLLM）"""

    def __init__(
        self,
        llm_gateway: LLMGateway,
        memory_store: LongTermMemoryStore,
        vector_store: VectorStore,
    ):
        self.llm_gateway = llm_gateway
        self.memory_store = memory_store
        self.vector_store = vector_store

        # 创建检查点（使用 LangGraph）
        self.checkpointer = PostgresSaver.from_conn_string(
            settings.DATABASE_URL
        )

        # 创建对话记忆（使用 LiteLLM Gateway）
        self.conversation_memory = ConversationMemory(
            llm_gateway=llm_gateway,
            memory_type="summary"  # 使用摘要记忆
        )

        # 构建图
        self.graph = self._build_graph()

    def _build_graph(self) -> StateGraph:
        """构建 Agent 图"""
        from typing import TypedDict, Annotated
        from langchain_core.messages import BaseMessage
        import operator

        class AgentState(TypedDict):
            messages: Annotated[list[BaseMessage], operator.add]
            user_id: str
            session_id: str

        builder = StateGraph(AgentState)

        # 添加节点
        builder.add_node("recall_memory", self._recall_long_term_memory)
        builder.add_node("process", self._process_message)
        builder.add_node("extract_memory", self._extract_memory)

        # 添加边
        builder.add_edge(START, "recall_memory")
        builder.add_edge("recall_memory", "process")
        builder.add_edge("process", "extract_memory")
        builder.add_edge("extract_memory", END)

        # 编译图（使用检查点）
        return builder.compile(checkpointer=self.checkpointer)

    async def _recall_long_term_memory(self, state: AgentState) -> dict:
        """召回长期记忆"""
        user_id = state["user_id"]
        last_message = state["messages"][-1].content

        # 搜索相关记忆
        memories = await self.memory_store.search(
            user_id=user_id,
            query=last_message,
            limit=5
        )

        # 将记忆添加到系统提示
        if memories:
            memory_text = "\n".join([
                f"- {m['content']}" for m in memories
            ])
            system_message = f"""相关记忆：
{memory_text}

请基于这些记忆回答用户问题。"""

            from langchain_core.messages import SystemMessage
            return {
                "messages": [SystemMessage(content=system_message)]
            }

        return {}

    async def _process_message(self, state: AgentState) -> dict:
        """处理消息"""
        # 加载对话记忆
        memory_vars = self.conversation_memory.load_memory_variables({})
        history = memory_vars.get("history", [])

        # 构建消息列表（转换为 LiteLLM 格式）
        lite_messages = []
        for msg in history:
            if hasattr(msg, 'content'):
                role = "user" if msg.__class__.__name__ == "HumanMessage" else "assistant"
                lite_messages.append({"role": role, "content": msg.content})

        # 添加当前消息
        last_msg = state["messages"][-1]
        if hasattr(last_msg, 'content'):
            lite_messages.append({"role": "user", "content": last_msg.content})

        # 调用 LiteLLM Gateway
        response = await self.llm_gateway.chat(messages=lite_messages)

        # 保存上下文
        self.conversation_memory.save_context(
            inputs={"input": last_msg.content if hasattr(last_msg, 'content') else str(last_msg)},
            outputs={"output": response.content or ""}
        )

        from langchain_core.messages import AIMessage
        return {
            "messages": [AIMessage(content=response.content or "")]
        }

    async def _extract_memory(self, state: AgentState) -> dict:
        """提取并存储长期记忆"""
        # 使用 LLM 判断是否需要存储
        # ... 提取逻辑 ...

        # 存储到长期记忆
        # await self.memory_store.put(...)

        return {}

    async def run(
        self,
        user_input: str,
        user_id: str,
        session_id: str,
    ):
        """运行 Agent"""
        from langchain_core.messages import HumanMessage

        config = {
            "configurable": {
                "thread_id": session_id,  # 使用 session_id 作为 thread_id
            }
        }

        result = await self.graph.ainvoke(
            {
                "messages": [HumanMessage(content=user_input)],
                "user_id": user_id,
                "session_id": session_id,
            },
            config=config
        )

        return result["messages"][-1].content
```

---

## 四、迁移计划

### 4.1 阶段 1: 准备工作（1-2 天）

1. **安装依赖**（已安装）:
   - `langgraph>=1.0.0` ✅
   - `langchain-openai>=1.0.0` ✅
   - 需要添加: `langgraph-checkpoint-postgres` 或使用内置的

2. **数据库迁移**:
   - LangGraph 的 PostgresSaver 需要特定的表结构
   - 运行迁移脚本创建必要的表

3. **配置更新**:
   - 更新配置文件，添加记忆相关配置

### 4.2 阶段 2: 实现核心功能（3-5 天）

1. **实现 LongTermMemoryStore**:
   - 创建 `backend/core/memory/langgraph_store.py`
   - 实现搜索、存储、删除功能
   - 编写单元测试

2. **实现 ConversationMemory**:
   - 创建 `backend/core/memory/langchain_memory.py`
   - 集成 LangChain Memory 类型
   - 编写单元测试

3. **更新 Checkpointer**:
   - 替换为 LangGraph 的 PostgresSaver/RedisSaver
   - 保持接口兼容性

### 4.3 阶段 3: 集成到 Agent Engine（2-3 天）

1. **更新 Agent Engine**:
   - 使用 LangGraph StateGraph
   - 集成 Checkpointer 和 Store
   - 集成 ConversationMemory

2. **更新 API 层**:
   - 更新 API 端点以使用新的记忆系统
   - 保持向后兼容

### 4.4 阶段 4: 测试和优化（2-3 天）

1. **功能测试**:
   - 测试短期记忆（检查点）
   - 测试长期记忆（Store）
   - 测试对话记忆（LangChain Memory）

2. **性能测试**:
   - 测试记忆检索性能
   - 测试检查点保存/加载性能
   - 优化慢查询

3. **迁移数据**:
   - 将现有记忆数据迁移到新系统
   - 验证数据完整性

---

## 五、代码示例

### 5.1 完整示例：使用 LangGraph + LangChain + LiteLLM 的 Agent

```python
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.postgres import PostgresSaver
from langgraph.store.postgres import PostgresStore
from langchain.memory import ConversationSummaryMemory
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from typing import TypedDict, Annotated
import operator

from core.llm.gateway import LLMGateway
from db.vector import VectorStore, get_vector_store
from core.memory.langgraph_store import LongTermMemoryStore
from core.memory.langchain_memory import ConversationMemory, LiteLLMChatModelAdapter

# 1. 定义状态
class AgentState(TypedDict):
    messages: Annotated[list[BaseMessage], operator.add]
    user_id: str
    session_id: str
    recalled_memories: list[dict]

# 2. 初始化组件
llm_gateway = LLMGateway(config=llm_config)  # 使用 LiteLLM
vector_store = get_vector_store()  # Qdrant 或 Chroma

# 3. 创建检查点（短期记忆）
checkpointer = PostgresSaver.from_conn_string(settings.DATABASE_URL)

# 4. 创建长期记忆存储（混合架构）
memory_store = LongTermMemoryStore(
    llm_gateway=llm_gateway,
    vector_store=vector_store
)
await memory_store.setup()

# 5. 创建对话记忆（使用 LiteLLM 适配器）
llm_adapter = LiteLLMChatModelAdapter(llm_gateway)
conversation_memory = ConversationSummaryMemory(
    llm=llm_adapter,
    return_messages=True,
    memory_key="history"
)

# 5. 定义节点
async def recall_memory(state: AgentState):
    """召回长期记忆（使用向量搜索）"""
    user_id = state["user_id"]
    query = state["messages"][-1].content

    # 使用向量搜索（Qdrant/Chroma）
    memories = await memory_store.search(
        user_id=user_id,
        query=query,
        limit=5
    )

    return {"recalled_memories": memories}

async def process(state: AgentState):
    """处理消息（使用 LiteLLM Gateway）"""
    # 加载对话记忆
    memory_vars = conversation_memory.load_memory_variables({})
    history = memory_vars.get("history", [])

    # 构建提示
    system_prompt = "你是一个有用的助手。"
    if state["recalled_memories"]:
        memory_text = "\n".join([m["content"] for m in state["recalled_memories"]])
        system_prompt += f"\n\n相关记忆：\n{memory_text}"

    # 构建消息列表（LiteLLM 格式）
    lite_messages = [{"role": "system", "content": system_prompt}]

    # 添加历史消息
    for msg in history:
        if hasattr(msg, 'content'):
            role = "user" if msg.__class__.__name__ == "HumanMessage" else "assistant"
            lite_messages.append({"role": role, "content": msg.content})

    # 添加当前消息
    last_msg = state["messages"][-1]
    if hasattr(last_msg, 'content'):
        lite_messages.append({"role": "user", "content": last_msg.content})

    # 调用 LiteLLM Gateway
    response = await llm_gateway.chat(messages=lite_messages)

    # 保存上下文
    conversation_memory.save_context(
        inputs={"input": last_msg.content if hasattr(last_msg, 'content') else str(last_msg)},
        outputs={"output": response.content or ""}
    )

    return {"messages": [AIMessage(content=response.content or "")]}

async def extract_memory(state: AgentState):
    """提取长期记忆"""
    # 判断是否需要存储
    # ... 提取逻辑 ...

    # 存储到 Store
    # await store.put(...)

    return {}

# 6. 构建图
builder = StateGraph(AgentState)
builder.add_node("recall_memory", recall_memory)
builder.add_node("process", process)
builder.add_node("extract_memory", extract_memory)

builder.add_edge(START, "recall_memory")
builder.add_edge("recall_memory", "process")
builder.add_edge("process", "extract_memory")
builder.add_edge("extract_memory", END)

# 7. 编译图（使用检查点）
# 注意：Store 不直接传给 compile，而是在节点中使用
graph = builder.compile(
    checkpointer=checkpointer  # 短期记忆
)

# 8. 使用
config = {"configurable": {"thread_id": "session-123"}}
result = await graph.ainvoke(
    {
        "messages": [HumanMessage(content="你好")],
        "user_id": "user-456",
        "session_id": "session-123",
        "recalled_memories": []
    },
    config=config
)
```

### 5.2 记忆提取示例

```python
async def extract_and_store_memory(
    conversation: list[dict],
    user_id: str,
    memory_store: LongTermMemoryStore,
    llm_gateway: LLMGateway
):
    """从对话中提取并存储记忆（使用 LiteLLM）"""
    import json
    import uuid

    # 使用 LiteLLM 提取记忆
    prompt = f"""从以下对话中提取重要的、值得长期记住的信息。

对话内容:
{format_conversation(conversation)}

请提取以下类型的信息:
1. 用户偏好 (preferences)
2. 重要事实 (facts)
3. 关键决策 (decisions)

以 JSON 数组格式返回，如果没有值得记住的信息，返回空数组 []。"""

    response = await llm_gateway.chat(
        messages=[{"role": "user", "content": prompt}]
    )

    memories = json.loads(response.content or "[]")

    # 存储到记忆存储（同时存储到 Store 和向量数据库）
    for memory in memories:
        await memory_store.put(
            user_id=user_id,
            memory_type=memory["type"],
            content=memory["content"],
            importance=float(memory.get("importance", 5)),
            metadata={"extracted": True}
        )

    return memories
```

---

## 六、最佳实践

### 6.1 记忆管理策略

1. **短期记忆（Checkpointer）**:
   - 用于保存当前会话的状态和对话历史
   - 自动管理，无需手动清理
   - 使用 `thread_id` 隔离不同会话

2. **长期记忆（Store）**:
   - 只存储重要的、跨会话的信息
   - 使用命名空间组织（按用户、类型等）
   - 定期清理过时或低重要性的记忆

3. **对话记忆（LangChain Memory）**:
   - 用于管理对话上下文，防止超出 token 限制
   - 根据场景选择合适的记忆类型
   - 定期摘要或截断历史

### 6.2 性能优化

1. **记忆检索**:
   - 使用向量数据库（Qdrant/Chroma）进行语义搜索
   - 限制检索数量（top_k）
   - 缓存常用记忆
   - 使用 LiteLLM 统一生成嵌入向量

2. **检查点**:
   - 合理设置检查点频率
   - 使用 Redis 提高性能（可选）
   - 定期清理旧检查点

3. **对话记忆**:
   - 使用摘要记忆节省 token
   - 设置合理的窗口大小
   - 避免重复存储相同信息
   - 通过 LiteLLM 适配器统一 LLM 调用

4. **架构优势**:
   - 向量搜索使用专用向量数据库（Qdrant/Chroma），性能优于 PostgreSQL 向量扩展
   - LiteLLM 统一管理多模型，支持 Fallback 和重试
   - Store 仅用于元数据，不承担向量搜索负担
   - 未来可统一使用 LiteLLM 生成嵌入向量（目前 VectorStore 使用 OpenAI 客户端）

5. **嵌入向量生成**:
   - 当前：VectorStore 内部使用 OpenAI 客户端生成嵌入
   - 未来：可改为通过 LiteLLM 统一生成，支持多模型和 Fallback
   - 建议：在 VectorStore 中添加 LiteLLM 支持，或创建统一的 EmbeddingService

### 6.3 错误处理

1. **记忆检索失败**:
   - 降级到关键词搜索
   - 返回空记忆而非错误

2. **检查点保存失败**:
   - 记录日志
   - 尝试重试
   - 使用内存备份

3. **Store 操作失败**:
   - 捕获异常
   - 记录到日志
   - 不影响主流程

---

## 七、总结

### 7.1 优势

1. **标准化**: 使用 LangChain/LangGraph 标准组件，减少自定义代码
2. **功能完整**: 利用框架提供的优化和最佳实践
3. **易于维护**: 跟随框架更新，减少维护成本
4. **生态集成**: 与 LangGraph 生态无缝集成

### 7.2 注意事项

1. **数据迁移**: 需要将现有记忆数据迁移到新系统
2. **接口兼容**: 保持 API 接口向后兼容
3. **性能测试**: 充分测试新系统的性能
4. **文档更新**: 更新相关文档和示例
5. **LiteLLM 适配**: 需要创建适配器使 LiteLLM Gateway 兼容 LangChain 接口
6. **向量数据库**: 保持使用独立的向量数据库（Qdrant/Chroma），不耦合到 PostgreSQL
7. **混合架构**: Store 用于元数据，向量数据库用于搜索，各司其职

### 7.3 下一步

1. 按照迁移计划逐步实施
2. 充分测试每个阶段
3. 收集用户反馈并优化
4. 持续关注 LangChain/LangGraph 更新

---

## 八、业界最新趋势（2025-2026）

### 8.1 条件触发 vs 全量调用

业界趋势是**不再每次对话都全量更新/检索记忆**，而是采用事件触发或查询复杂度判断：

| 系统 | 策略 | 特点 |
|-----|------|------|
| **MemGPT/Letta** | Agent 自主决定 | 通过工具调用决定何时存储/检索 |
| **SimpleMem** | 信息密度过滤 | novelty_threshold + 查询复杂度自适应 |
| **ENGRAM** | 分类型记忆 | episodic/semantic/procedural 分开处理 |

### 8.2 混合检索（Hybrid Search）

现代记忆系统普遍采用**混合检索**，结合多种方法：

```
┌─────────────────────────────────────────────────────────────┐
│                    混合检索架构                              │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│   ┌───────────────┐   ┌───────────────┐   ┌──────────────┐ │
│   │  BM25/关键词   │   │  向量相似度    │   │  元数据过滤  │ │
│   │  词法匹配      │   │  语义匹配      │   │  符号匹配    │ │
│   │  (类似 egrep) │   │  (embedding)  │   │  (filter)   │ │
│   └───────────────┘   └───────────────┘   └──────────────┘ │
│           │                   │                   │         │
│           └───────────────────┼───────────────────┘         │
│                               ▼                             │
│                      ┌──────────────────┐                   │
│                      │  RRF 融合排序     │                   │
│                      └──────────────────┘                   │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

**关于 "egrep" 相关工具**：

用户提到的 "egrep" 在记忆系统中对应的是 **BM25/关键词检索**：
- **BM25**: 基于词频的经典检索算法，类似 grep/egrep 的原理
- **用途**: 快速过滤 + 精确匹配，与向量检索互补
- **当前实现**: SimpleMem 已集成 BM25 检索 (`rank_bm25` 库)

```python
# backend/core/memory/simplemem_client.py
from rank_bm25 import BM25Okapi

class SimpleMemAdapter:
    def _bm25_search(self, session_id: str, query: str, k: int):
        """BM25 词法检索（类似 egrep 的作用）"""
        tokenized_query = query.split()
        scores = self._bm25_index[session_id].get_scores(tokenized_query)
        # ... 返回 top-k 结果

    def _reciprocal_rank_fusion(self, semantic, lexical, k):
        """RRF 融合向量检索和 BM25 检索结果"""
        # 语义结果 + 词法结果 → 融合排序
```

### 8.3 分层记忆架构

```
┌─────────────────────────────────────────────────────────────┐
│                     分层记忆架构                             │
├─────────────────────────────────────────────────────────────┤
│  工作记忆 (Working)   │ 当前对话上下文      │ Checkpointer │
│  短期记忆 (Short)     │ 会话内长程记忆      │ SimpleMem    │
│  长期记忆 (Long)      │ 跨会话用户偏好      │ 待实现       │
└─────────────────────────────────────────────────────────────┘
```

---

## 九、优化建议

### 9.1 短期（无需改架构）

1. **检索阶段添加跳过逻辑**：简单问候不检索记忆
2. **统一 extract_memory 和 SimpleMem**：避免重复提取
3. **调整配置**：根据场景选择成本/质量平衡

### 9.2 中期

1. **添加用户级长期记忆**：跨会话的重要信息自动提升
2. **更细粒度的复杂度判断**：不同复杂度使用不同策略

### 9.3 长期

1. **Agent 自主记忆管理**：将 store/search memory 作为工具暴露
2. **类型化记忆**：区分事实型/偏好型/技能型记忆

---

## 十、参考资料

- [LangGraph Memory 文档](https://langchain-ai.lang.chat/langgraph/concepts/memory/)
- [LangGraph Checkpoint 文档](https://langchain-ai.lang.chat/langgraph/how-tos/memory/add-memory/)
- [LangChain Memory 文档](https://python.langchain.com/docs/modules/memory/)
- [LangMem SDK](https://blog.langchain.com/langmem-sdk-launch)
- [SimpleMem 论文](https://arxiv.org/abs/2601.02553) - 30x Token 压缩
- [ENGRAM](https://arxiv.org/abs/2511.12960) - 类型化记忆系统
- [MemGPT/Letta](https://github.com/letta-ai/letta) - Agent 自主记忆管理
